{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8401723,"sourceType":"datasetVersion","datasetId":4999080}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-15T13:10:06.873616Z","iopub.execute_input":"2024-05-15T13:10:06.874075Z","iopub.status.idle":"2024-05-15T13:10:06.886604Z","shell.execute_reply.started":"2024-05-15T13:10:06.87404Z","shell.execute_reply":"2024-05-15T13:10:06.885321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Part II : Poisson Regression**","metadata":{}},{"cell_type":"code","source":"# get the data\n# transform days values to a range of integers\n# transform the hours to one hot encoding \n# tansform the days to a label encoding\n# introduce GLM theory\n# fit a glm model to the data\n# explain the results \n    # explain the p-values of the z-scores\n    # explain the deviance test \n    # explain the goodness of fit test\n# interpret the coefficients \n# plot the coefficents as a function of the hour and Base\n# plot the counts and the predicted values Lambda against the days \n# plot a the Pearson residual against the fitted values\n# identify the outliers and the points with high leverage and remove them\n# identify what transformed version of the model can make the fit any better if the relationship is not linear\n# do a bootstapping validation\n    # for every version of the model :\n        # fit the model onto 30 resampled subsets\n        # get statistics like goodness of fit test values of all the 30 fits\n        # draw a box plot of the values \n\n# make a conclusion about the best model\n\n# fit a deep learning model\n# calculate a goodness of fit of deep model","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:06.889688Z","iopub.execute_input":"2024-05-15T13:10:06.890119Z","iopub.status.idle":"2024-05-15T13:10:06.901598Z","shell.execute_reply.started":"2024-05-15T13:10:06.890088Z","shell.execute_reply":"2024-05-15T13:10:06.900265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nUber = pd.read_csv('/kaggle/input/uber-pickups-summary/UberPickupsSummary (2).csv')\nUber = Uber.drop(columns=['Unnamed: 0'])\nUber","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:06.90342Z","iopub.execute_input":"2024-05-15T13:10:06.904432Z","iopub.status.idle":"2024-05-15T13:10:06.954534Z","shell.execute_reply.started":"2024-05-15T13:10:06.904396Z","shell.execute_reply":"2024-05-15T13:10:06.952607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_labels, unique_values = pd.factorize(Uber['Date'])\nUber['Date'] = encoded_labels\nUber = pd.get_dummies(Uber, columns=['Base', 'hour']).drop(columns=['hour_0', 'Base_B02512'])\nUber['intercept'] = [1]*len(Uber)\nUber = Uber.astype(np.float32)\nUber","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:06.957566Z","iopub.execute_input":"2024-05-15T13:10:06.958064Z","iopub.status.idle":"2024-05-15T13:10:07.046524Z","shell.execute_reply.started":"2024-05-15T13:10:06.958009Z","shell.execute_reply":"2024-05-15T13:10:07.045247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# An introduction to Generalize linear models Theorie\n\nIn simple linear regression , we regress the dependent variable Y onto the predictors X , using the formula $Y = X.\\beta + \\epsilon$ , such that Y is real number vector that follow as normal distribution $N(\\mu,\\sigma)$ and $\\beta$ is the coefficients vector that we try to find by minimizing the the error vector $\\epsilon$ using Least squares method, thus making the predicted values $X.\\beta$ as close as possible to the real Y (i.e $Y \\approx X.\\beta$ ). \n\nSounds great!, it turned out that this method doesn't work well when Y in not real number, this means that Y doesn't follow the normal distribution discribed above, so what can we do ? . To solve this problem we need go a step back to the linear regression problem and make small clarification that we will take as a starting point for our startegie.\n\nWhen we said that $Y \\approx X.\\beta$ that was not precise enough,  in reality the correct formula should be $\\mu = E[Y|X] \\approx X.\\beta$\nthis means that not every point Y is linear with the predictors X , but more precisely it is the conditional mean of Y for each value of X that should be in a linear relationship with X\n\nWith that being clarified,  we can start discovering the theoey of Generalized Linear Model \"GLM\"\n\n**1. Modeling the probability distribution:**\nWhen Y doesn't follow a normal distribution , we need first to figure what distribution it follows , one case is the classification problem where   $Y$ ~ $Bernoulli(p)$, where p is the mean of the distribution, using the idea discribed above about relationship of the mean and the predictors , instead of modeling $Y \\approx X.\\beta$, we can model \n$p \\approx X.\\beta$ , but $0 \\leq p \\leq 1$ ,which means we'll have some problems when modeling it with $X.\\beta$  that ranges in $[-\\infty, +\\infty]$ , so we need to transforme p (or $X.\\beta$) so that both sides of the equation can only have values from the same range, and then we model that tranformation as $$transform(p) \\approx X.\\beta$$.\nThe tranformation that is used in the classification problem is the one that is known as the **logit** tranformation\n$$logit(p) = \\log{\\frac{p}{1-p}}$$\n","metadata":{}},{"cell_type":"code","source":"y = Uber['size']\nX = Uber.drop(columns=['size'])\nmodel = sm.GLM(y, X, family = sm.families.Poisson())\nresults = model.fit()\nprint(results.summary())","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:07.048663Z","iopub.execute_input":"2024-05-15T13:10:07.049158Z","iopub.status.idle":"2024-05-15T13:10:07.375792Z","shell.execute_reply.started":"2024-05-15T13:10:07.04911Z","shell.execute_reply":"2024-05-15T13:10:07.374611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explaining the test results\n\n**1. The Deviance Test:**\nThe deviance test is analoguous to the F-test in linear regression\nThe very high values of the deviance $(1.1402*10^6)$ indicates that thers is enough eivdances to reject null hypothesis that $\\beta = 0$ and accept the alternative that says that at least one of the coefficents is significantly different than zero\n\n\n**2. The Z-test:**\nThe p-values of the z-scores are all lower than $5%$ Which shows that all the coefficents are significantly different than zero\n\n**3. The Pearson $\\chi^2$-test:**\nThis the goodness of fit test that tests how well the actual counts fit the predicted probabilities. $1.30*10^6$ is a large values for this test showing that there is a significant fit between the predicted probabilites and the actual Poisson count random variables in the columns 'size'\n\n\n# Interpreting the Coefficients\n\nWe should be careful when interpreting the results, because an interpretation can differ when the coefficient of a categorical feature from when the feature has real values \n\n1. Coefficient of date feature:\nthe date feature in this dataset is an integer value tha can range from $0$ to $+\\infty$ which means that our interpretation of the coefficient will the same as we do with a real values feature , in this case we can that $\\beta_{Date} = 0.0036$ means that for every one day step to the future we expect an addition of approximitly 0.0036 in $E[Size]$ the expected value of  ","metadata":{}},{"cell_type":"code","source":"columns = ['hour_'+str(i) for i in range(1,24)]\nhour_coefs = results.params[columns]\nsns.relplot(x = range(1,24), y= hour_coefs.values, kind='line', marker='o')\nplt.xlabel('hours')\nplt.ylabel('count rate')\ncolumns = [\"Base_B02598\", \"Base_B02617\", \"Base_B02682\", \"Base_B02764\"]\nBase_coefs = results.params[columns]\nsns.relplot(x = columns, y= Base_coefs.values, kind='line', marker='o')\nplt.xlabel('base')\nplt.ylabel('count rate')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:07.377625Z","iopub.execute_input":"2024-05-15T13:10:07.378386Z","iopub.status.idle":"2024-05-15T13:10:08.509771Z","shell.execute_reply.started":"2024-05-15T13:10:07.37834Z","shell.execute_reply":"2024-05-15T13:10:08.508625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.DataFrame(results.fittedvalues, columns=['fitted_values'])\ndata['Date'] = X['Date']\ndata['counts'] = y.values\nfig, ax = plt.subplots(figsize=(20,10))\nsns.scatterplot(data = data, x = 'Date', y='counts',ax=ax, label='counts')\ndata_grouped = data.groupby(by='Date').mean()\nsns.lineplot(data = data_grouped, x='Date', y='fitted_values',ax=ax ,color='r', label='fitted_values')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:08.511768Z","iopub.execute_input":"2024-05-15T13:10:08.51258Z","iopub.status.idle":"2024-05-15T13:10:09.548942Z","shell.execute_reply.started":"2024-05-15T13:10:08.512539Z","shell.execute_reply":"2024-05-15T13:10:09.548002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resid = (data['counts'] - data['fitted_values'])\n# /np.sqrt(data['fitted_values'])\nfig, ax = plt.subplots()\nsns.set_theme()\nsns.scatterplot(x= data['fitted_values'], y= resid, ax=ax)\nax.set_xlabel('fitted_values')\nax.set_ylabel('residual')\nax.axhline(0, linewidth=1.3, color='red', linestyle='--')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:09.550272Z","iopub.execute_input":"2024-05-15T13:10:09.551168Z","iopub.status.idle":"2024-05-15T13:10:10.153138Z","shell.execute_reply.started":"2024-05-15T13:10:09.551133Z","shell.execute_reply":"2024-05-15T13:10:10.152017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# algorithm\n# models_list = [xgboost.XGRegressor, deepNN, SVR, GLM.poisson, KNN(1), KNN(k)]\n# train each model on the train set\n# generate 30 sample from the test set using bootstrapping \n# test each model in the model list on the 30 samples\n# calculate the average chi2 square metric for each model's prediction\n# drow a box plot of the calculated chi2 values \n# choose the best model","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:10.15483Z","iopub.execute_input":"2024-05-15T13:10:10.155585Z","iopub.status.idle":"2024-05-15T13:10:10.161212Z","shell.execute_reply.started":"2024-05-15T13:10:10.155543Z","shell.execute_reply":"2024-05-15T13:10:10.159782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\n\ninput_data = Uber.drop(columns=['size']).values\nscaler = StandardScaler()\ninput_data = scaler.fit_transform(input_data)\noutput_data = Uber['size'].values\n\nclass PearsonGoodnessOfFit(tf.keras.metrics.Metric):\n    def __init__(self, name='pearson chi squared distance', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.pearson_goodness_of_fit = self.add_weight(\n            shape=(),\n            initializer='zeros',\n            name='pearson_goodness_of_fit'\n        )\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        \n\n        if sample_weight is not None:\n            pass\n\n        self.pearson_goodness_of_fit.assign(tf.reduce_mean(tf.square(y_true - y_pred)/y_pred))\n\n    def result(self):\n        return self.pearson_goodness_of_fit\n\n    \n    \n    \n    \n    \ndeep_model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='exponential')\n])\n\ndeep_model.compile(\n    loss = tf.keras.losses.Poisson(),\n    metrics = [tf.keras.metrics.MeanSquaredError()],\n    optimizer = tf.keras.optimizers.Adam()\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:10.16722Z","iopub.execute_input":"2024-05-15T13:10:10.167735Z","iopub.status.idle":"2024-05-15T13:10:10.208587Z","shell.execute_reply.started":"2024-05-15T13:10:10.16769Z","shell.execute_reply":"2024-05-15T13:10:10.207282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom scipy.stats import poisson\nfrom scipy.stats import chisquare\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain_set_x, test_set_x, train_set_y, test_set_y=train_test_split(input_data,output_data, test_size=0.2) \n\n\n\ndef bootstrap_samples(test_set_x, test_set_y, n_samples):\n    \"\"\"\n    Perform bootstrapping on two datasets.\n\n    Parameters:\n        test_set_x (array-like): The input data set X.\n        test_set_y (array-like): The output data set Y.\n        n_samples (int): Number of samples to generate.\n\n    Returns:\n        x_samples (list): List of bootstrapped samples from test_set_x.\n        y_samples (list): List of bootstrapped samples from test_set_y.\n    \"\"\"\n#     print(test_set_x)\n    x_samples = []\n    y_samples = []\n\n    for _ in range(n_samples):\n        # Generate random indices with replacement\n        indices = np.random.randint(0, len(test_set_x), len(test_set_x))\n#         print(indices)\n        # Create bootstrapped samples for test_set_x and test_set_y\n        x_bootstrap = test_set_x[indices] \n        y_bootstrap = test_set_y[indices] \n\n        # Append the bootstrapped samples to the lists\n        x_samples.append(x_bootstrap)\n        y_samples.append(y_bootstrap)\n\n    return x_samples, y_samples\n\n\n\n\n\n\ndef chi_squared_distance(y_pred, y_true):\n    \"\"\"\n    Calculate the Chi-squared distance between observed and expected values.\n\n    Parameters:\n        y_pred (array-like): Lambda parameter for Poisson distribution.\n        y_true (array-like): Observed counts following Poisson distribution.\n\n    Returns:\n        chi2_dist (float): Chi-squared distance between observed and expected values.\n    \"\"\"\n    y_pred = y_pred.reshape(-1,1)\n#     print(np.sum(y_pred < 0))\n    y_true = y_true.reshape(-1,1)\n    \n    # Calculate the expected probabilities using y_pred\n    expected_probs = poisson.pmf(y_true, y_pred)\n#     print(np.sum(np.isnan(expected_probs)))\n    # Normalize the expected probabilities\n#     expected_probs_normalized = expected_probs / np.sum(expected_probs)\n\n    # Compute the Chi-squared distance\n#     chi2_dist, _ = chisquare(y_true, f_exp=expected_probs_normalized)\n    f_obs = np.ones(y_true.shape)\n    chi2_dist = (f_obs - expected_probs)**2\n    score = np.mean(chi2_dist)\n#     print(score)\n    return score\n\n\n\n\ndef draw_boxplot(scores_doubly_linked_list, model_names):\n    \"\"\"\n    Draw a box plot of scores for different models on different test sets.\n\n    Parameters:\n        scores_doubly_linked_list (list): Doubly linked list containing lists of scores.\n        model_names (list): Names of the models.\n\n    Returns:\n        None\n    \"\"\"\n    \n    # Flatten the doubly linked list to a list of scores and corresponding model names\n    flat_scores = []\n    flat_model_names = []\n#     print(len(scores_doubly_linked_list))\n#     print(len(model_names))\n    for i in range(len(scores_doubly_linked_list)):\n        for j in range(len(scores_doubly_linked_list[i])):\n            flat_scores.append(scores_doubly_linked_list[i][j])\n            flat_model_names.append(model_names[i]) \n\n    # Create a DataFrame from the flattened data\n    df = pd.DataFrame({'Model': flat_model_names, 'Score': flat_scores})\n\n    # Draw a box plot using Seaborn\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Model', y='Score', data=df)\n    plt.title('Box Plot of Scores for Different Models')\n    plt.xlabel('Model')\n    plt.ylabel('Score')\n    plt.xticks(rotation=45)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:10.210498Z","iopub.execute_input":"2024-05-15T13:10:10.210871Z","iopub.status.idle":"2024-05-15T13:10:10.234682Z","shell.execute_reply.started":"2024-05-15T13:10:10.210831Z","shell.execute_reply":"2024-05-15T13:10:10.233553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"deep_model.fit(train_set_x, train_set_y, epochs=10, batch_size=5000, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:10.23604Z","iopub.execute_input":"2024-05-15T13:10:10.236591Z","iopub.status.idle":"2024-05-15T13:10:15.352191Z","shell.execute_reply.started":"2024-05-15T13:10:10.236554Z","shell.execute_reply":"2024-05-15T13:10:15.351193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scm = []\n# for i in range(len(test_sets_x)):\n#     y_pred = deep_model.predict(test_sets_x[i])\n#     y_true = np.array(test_sets_y[i]).reshape(-1,1)\n# #     print(tf.keras.losses.MSE(test_sets_y[i], y_pred))\n#     scm.append(np.mean(np.square(y_pred - y_true)))\n# print(scm)\n# scores.append(scm)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:15.353344Z","iopub.execute_input":"2024-05-15T13:10:15.353986Z","iopub.status.idle":"2024-05-15T13:10:15.358184Z","shell.execute_reply.started":"2024-05-15T13:10:15.353952Z","shell.execute_reply":"2024-05-15T13:10:15.357233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(len(test_sets_x)):\n#     deep_model.evaluate(test_sets_x[i], test_sets_y[i])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:15.359828Z","iopub.execute_input":"2024-05-15T13:10:15.360336Z","iopub.status.idle":"2024-05-15T13:10:15.386353Z","shell.execute_reply.started":"2024-05-15T13:10:15.360293Z","shell.execute_reply":"2024-05-15T13:10:15.385148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deep_model.fit(train_set_x, train_set_y, epochs = 10, batch_size=5000, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:15.388085Z","iopub.execute_input":"2024-05-15T13:10:15.388945Z","iopub.status.idle":"2024-05-15T13:10:15.404138Z","shell.execute_reply.started":"2024-05-15T13:10:15.388877Z","shell.execute_reply":"2024-05-15T13:10:15.402855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svr = SVR()\nxgb = XGBRegressor()\nknn_1 = KNeighborsRegressor(n_neighbors=1)\nknn_5 = KNeighborsRegressor(n_neighbors=5)\n\n               \n#                sm.GLM(train_y, train_X, family = sm.families.Poisson()) ]\n\n# deep_model.fit(train_set_x, train_set_y, epochs = 10, batch_size=1000)\nsvr.fit(train_set_x, train_set_y)\nxgb.fit(train_set_x, train_set_y)\nknn_1.fit(train_set_x, train_set_y)\nknn_5.fit(train_set_x, train_set_y)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:15.406093Z","iopub.execute_input":"2024-05-15T13:10:15.406987Z","iopub.status.idle":"2024-05-15T13:10:42.305653Z","shell.execute_reply.started":"2024-05-15T13:10:15.406943Z","shell.execute_reply":"2024-05-15T13:10:42.304494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sets_x, test_sets_y = bootstrap_samples(test_set_x, test_set_y, 10)\nprint(test_sets_y[2].shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:42.310011Z","iopub.execute_input":"2024-05-15T13:10:42.31139Z","iopub.status.idle":"2024-05-15T13:10:42.320429Z","shell.execute_reply.started":"2024-05-15T13:10:42.311342Z","shell.execute_reply":"2024-05-15T13:10:42.319281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_list = [deep_model, \n#                svr,\n#                xgb,\n               knn_1,\n               knn_5]\nscores = []\nfor model in models_list:\n    scm = []\n    for i in range(len(test_sets_x)):\n        y_true = np.array(test_sets_y[i]).reshape(-1,1)\n        y_pred = model.predict(test_sets_x[i])\n        y_pred = np.array(y_pred).reshape(-1,1)\n#         print(np.mean(np.square(y_pred - test_sets_y[i])))\n        scm.append(np.mean(np.square(y_pred - y_true)))\n        \n    scores.append(scm)\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:42.321913Z","iopub.execute_input":"2024-05-15T13:10:42.322375Z","iopub.status.idle":"2024-05-15T13:10:50.594193Z","shell.execute_reply.started":"2024-05-15T13:10:42.322335Z","shell.execute_reply":"2024-05-15T13:10:50.593093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(scores)\ndraw_boxplot(scores ,['deep_model', 'KNN_1', 'KNN_5'])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:50.595478Z","iopub.execute_input":"2024-05-15T13:10:50.596469Z","iopub.status.idle":"2024-05-15T13:10:51.062758Z","shell.execute_reply.started":"2024-05-15T13:10:50.596431Z","shell.execute_reply":"2024-05-15T13:10:51.061647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poisson.pmf(1, -1)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T13:10:51.063881Z","iopub.execute_input":"2024-05-15T13:10:51.064299Z","iopub.status.idle":"2024-05-15T13:10:51.071734Z","shell.execute_reply.started":"2024-05-15T13:10:51.064265Z","shell.execute_reply":"2024-05-15T13:10:51.07032Z"},"trusted":true},"execution_count":null,"outputs":[]}]}